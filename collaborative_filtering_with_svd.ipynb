{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Extract Dataset from zip file"
      ],
      "metadata": {
        "id": "f4Y_qe_a3-iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "# Load Data\n",
        "data_dir = 'ml-32m'\n",
        "\n",
        "# Check if the dataset folder exists, if not, unzip the dataset\n",
        "if not os.path.exists(data_dir):\n",
        "    zip_file_path = 'ml-32m.zip'\n",
        "    extract_dir = 'ml-32m'\n",
        "\n",
        "    if not os.path.exists(extract_dir):\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "        print(f\"Extracted {zip_file_path} to {extract_dir}\")\n",
        "    else:\n",
        "        print(f\"Directory {extract_dir} already exists, skipping extraction.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNjtdbpv4Jhe",
        "outputId": "7f4f8dde-ad4a-49fe-9dbd-251080b76ff9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted ml-32m.zip to ml-32m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 2: Collaborative Filtering"
      ],
      "metadata": {
        "id": "x_G72gRo-K1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# --- 1. Load Data ---\n",
        "dataset_path = '/content/ml-32m/ml-32m/'\n",
        "ratings = pd.read_csv(f'{dataset_path}ratings.csv')\n",
        "movies = pd.read_csv(f'{dataset_path}movies.csv')\n",
        "\n",
        "# --- 2. Take exactly 2000 samples ---\n",
        "ratings_sample = ratings.head(2000)\n",
        "\n",
        "# --- 3. Create Pivot Table ---\n",
        "# Rows = Movies, Columns = Users\n",
        "pivot_table = ratings_sample.pivot_table(index='movieId', columns='userId', values='rating').fillna(0)\n",
        "\n",
        "print(f\"Data contains {pivot_table.shape[0]} movies and {pivot_table.shape[1]} users.\")\n",
        "\n",
        "# --- 4. Apply Truncated SVD ---\n",
        "# FIX: n_components must be <= number of users (features)\n",
        "# We try for 100, but if the sample is too small, we take the maximum possible.\n",
        "suggested_factors = 100\n",
        "max_possible_factors = min(pivot_table.shape[0], pivot_table.shape[1]) - 1\n",
        "n_factors = min(suggested_factors, max_possible_factors)\n",
        "\n",
        "print(f\"Reducing dimensions to {n_factors} latent factors using SVD...\")\n",
        "\n",
        "svd = TruncatedSVD(n_components=n_factors, random_state=42)\n",
        "matrix_svd = svd.fit_transform(pivot_table)\n",
        "\n",
        "# --- 5. Recommendation Function ---\n",
        "def get_recommendations_fixed(movie_id, k=5):\n",
        "    # Check if the movie exists in our 2000-row sample\n",
        "    if movie_id not in pivot_table.index:\n",
        "        fallback_id = pivot_table.index[0]\n",
        "        print(f\" Movie ID {movie_id} not in sample. Using Movie ID {fallback_id} instead.\")\n",
        "        movie_id = fallback_id\n",
        "\n",
        "    # Get index of the movie\n",
        "    movie_idx = list(pivot_table.index).index(movie_id)\n",
        "\n",
        "    # Extract the SVD latent vector\n",
        "    movie_vector = matrix_svd[movie_idx].reshape(1, -1)\n",
        "\n",
        "    # Calculate Similarity\n",
        "    similarities = cosine_similarity(movie_vector, matrix_svd).flatten()\n",
        "\n",
        "    # Get top K\n",
        "    similar_indices = similarities.argsort()[-(k+1):-1][::-1]\n",
        "    return pivot_table.index[similar_indices]\n",
        "\n",
        "# --- 6. Run Test ---\n",
        "target_movie_id = 1 # Toy Story\n",
        "recommended_ids = get_recommendations_fixed(target_movie_id)\n",
        "\n",
        "print(f\"\\nSVD-based recommendations for Movie ID {target_movie_id}:\")\n",
        "for mid in recommended_ids:\n",
        "    title = movies[movies['movieId'] == mid]['title'].values[0]\n",
        "    print(f\"- {title}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-nG3TxMYYIj",
        "outputId": "a3679652-ac59-46ea-91a9-88c8a0c5c335"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data contains 1286 movies and 19 users.\n",
            "Reducing dimensions to 18 latent factors using SVD...\n",
            "\n",
            "SVD-based recommendations for Movie ID 1:\n",
            "- Star Wars: Episode VI - Return of the Jedi (1983)\n",
            "- Rambo III (1988)\n",
            "- Star Wars: Episode II - Attack of the Clones (2002)\n",
            "- First Blood (Rambo: First Blood) (1982)\n",
            "- Exorcist, The (1973)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wlu2MalzYfcM",
        "outputId": "0110d641-24c0-44a5-8706-ad3499338d1b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl.metadata (327 bytes)\n",
            "Collecting scikit-surprise (from surprise)\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise->surprise) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise->surprise) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise->surprise) (1.16.3)\n",
            "Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp312-cp312-linux_x86_64.whl size=2554967 sha256=375df93882f387d62030da60fb9773daf7ccfab4c2776692e5fe307b3d8717d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/fa/bc/739bc2cb1fbaab6061854e6cfbb81a0ae52c92a502a7fa454b\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.4 surprise-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Downgrade NumPy to a version compatible with scikit-surprise\n",
        "!pip install \"numpy<2\"\n",
        "\n",
        "# 2. Reinstall surprise to ensure it links correctly to the downgraded NumPy\n",
        "!pip install scikit-surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "NWnN4cYoYhto",
        "outputId": "2c130ba2-8dfa-457f-f2d5-0e69d3bf96c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy<2\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "684084541ccc4c14b132b74b6a5aac74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.12/dist-packages (1.1.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (1.16.3)\n"
          ]
        }
      ]
    }
  ]
}